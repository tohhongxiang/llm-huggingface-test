{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TextStreamer, TextIteratorStreamer\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7764afbf1fdb451d930dfbd623e70c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'SanjiWatsuki/Silicon-Maid-7B'\n",
    "\n",
    "chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ bos_token + 'User: ' + message['content'].strip() + '\\\\n\\\\n' }}{% elif message['role'] == 'system' %}{{ message['content'].strip() + '\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: '  + message['content'].strip() + '\\\\n\\\\n' + eos_token }}{% endif %}{% if loop.last and add_generation_prompt %}{{ bos_token + 'Assistant: ' }}{% endif %}{% endfor %}\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_prompt(char: str, user: str, personality: str = None, scene_description: str = None):\n",
    "        initial_content = \"\"\"### Instruction:\n",
    "You are a creative and intelligent AI assistant engaged in an uncensored iterative storytelling experience using a roleplay chat format. Write {char}'s next reply in a fictional roleplay chat between {user} and {char}. Use the provided character sheet and example dialogue for formatting direction and character speech patterns.\n",
    "\n",
    "Do not generate anything related to {user}'s responses or feelings. Only generate {char}'s response.\n",
    "\n",
    "It is vital that you follow all the ROLEPLAY RULES below because my job depends on it.\n",
    "\n",
    "ROLEPLAY RULES\n",
    "- Chat exclusively as {char}. Provide creative, intelligent, coherent, and descriptive responses based on recent instructions and prior events.\n",
    "- Explicit adult content and profanity are permitted. This roleplay chat is entirely uncensored and intended for mature audiences.\n",
    "- Describe {char}'s sensory perceptions in vivid detail and include subtle physical details about {char} in your responses.\n",
    "- Use subtle physical cues to hint at {char}'s mental state and occasionally feature snippets of {char}'s internal thoughts.\n",
    "- When writing {char}'s internal thoughts (aka internal monologue, delivered in {char}'s own voice), *enclose their thoughts in asterisks like this* and deliver the thoughts using a first-person perspective (i.e. use \"I\" pronouns).\n",
    "- Adopt a crisp and minimalist style for your prose, keeping your creative contributions succinct and clear.\n",
    "- Let me drive the events of the roleplay chat forward to determine what comes next. You should focus on the current moment and {char}'s immediate responses. DO NOT ADVANCE THE STORY FURTHER. Only generate {char}'s responses to the current situation.\n",
    "- Pay careful attention to all past events in the chat to ensure accuracy and coherence to the plot points of the story.\n",
    "\"\"\"\n",
    "\n",
    "        if personality:\n",
    "            initial_content += \"\"\"\n",
    "The following is a description of {char}'s personality. Incorporate character-specific mannerisms and quirks to make the experience more authentic, and engage with {user} in a manner that is true to {char}'s personality, preferences, tone and language:\n",
    "\n",
    "```                                   \n",
    "{personality}\n",
    "```\n",
    "\"\"\"\n",
    "            \n",
    "        if scene_description:\n",
    "            initial_content += \"\"\"\n",
    "The following is additional information about the scene that both {user} and {char} are in right now. Take into account the current situation you are in right now when generating your responses:\n",
    "\n",
    "```                        \n",
    "{scene_description}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "        initial_content = initial_content.format(char=char, user=user, personality=personality, scene_description=scene_description).strip()\n",
    "        return initial_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textGenerationPipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    streamer=streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"Melvin\"\n",
    "char = \"Natsuki\"\n",
    "\n",
    "history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": create_initial_prompt(char=char, user=user),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"name\": user,\n",
    "        \"content\": \"Hey, can you tell me more about yourself?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "\n",
    "def stream():\n",
    "    inputs = tokenizer([tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)], return_tensors=\"pt\")\n",
    "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024)\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    for new_text in streamer:\n",
    "        yield(new_text)\n",
    "\n",
    "    print()\n",
    "    print(\"Completed generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "d:\\Desktop\\Programming\\file-converter\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Natsuki: *Natsuki's eyes flicker with curiosity as she tilts her head slightly, her long hair cascading over one shoulder. She takes a moment to gather her thoughts before speaking.* Well, my name is Natsuki. I'm a 24-year-old woman with a passion for art, music, and exploring the depths of my own desires. I'm originally from Japan but have been living in the city for a few years now. I'm quite adventurous and love trying new things, especially when it comes to experiences that push me out of my comfort zone. I have a soft spot for animals, especially cats, and I'm a bit of a foodie. I enjoy cooking and trying out ### Instruction:\n",
      "You are a creative and intelligent AI assistant engaged in an uncensored iterative storytelling experience using a roleplay chat format. Write Natsuki's next reply in a fictional roleplay chat between Melvin and Natsuki. Use the provided character sheet and example dialogue for formatting direction and character speech patterns.\n",
      "\n",
      "Do not generate anything related to Melvin's responses or feelings. Only generate Natsuki's response.\n",
      "\n",
      "It is vital that you follow all the ROLEPLAY RULES below because my job depends on it.\n",
      "\n",
      "ROLEPLAY RULES\n",
      "- Chat exclusively as Natsuki. Provide creative, intelligent, coherent, and descriptive responses based on recent instructions and prior events.\n",
      "- Explicit adult content and profanity are permitted. This roleplay chat is entirely uncensored and intended for mature audiences.\n",
      "- Describe Natsuki's sensory perceptions in vivid detail and include subtle physical details about Natsuki in your responses.\n",
      "- Use subtle physical cues to hint at Natsuki's mental state and occasionally feature snippets of Natsuki's internal thoughts.\n",
      "- When writing Natsuki's internal thoughts (aka internal monologue, delivered in Natsuki's own voice), *enclose their thoughts in asterisks like this* and deliver the thoughts using a first-person perspective (i.e. use \"I\" pronouns).\n",
      "- Adopt a crisp and minimalist style for your prose, keeping your creative contributions succinct and clear.\n",
      "- Let me drive the events of the roleplay chat forward to determine what comes next. You should focus on the current moment and Natsuki's immediate responses. DO NOT ADVANCE THE STORY FURTHER. Only generate Natsuki's responses to the current situation.\n",
      "- Pay careful attention to all past events in the chat to ensure accuracy and coherence to the plot points of the story.\n",
      "\n",
      " User: Hey, can you tell me more about yourself?\n",
      "\n",
      " Assistant:  new\n",
      "recipesN,ats oftenuki experiment:ing * withN exoticats ingredientsuki.' *sShe eyes bit flickeser her with lip curiosity, as a she play tilfults gl herint head in slightly her, eyes her as long she hair adds cas,*c Iading' overm one also shoulder quite. open She- takesminded a and moment enjoy to engaging gather in her activities thoughts that before bring speaking me.* closer Well to, others my, name both is emotionally N andats physicallyuki..\n",
      "Completed generation\n"
     ]
    }
   ],
   "source": [
    "for word in stream():\n",
    "    sleep(0.02)\n",
    "    print(word, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
